{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teste para extrair via yahoo finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yfinance\n",
      "  Using cached yfinance-0.2.62-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\python312\\lib\\site-packages (from yfinance) (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\python312\\lib\\site-packages (from yfinance) (2.2.4)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\python312\\lib\\site-packages (from yfinance) (2.32.3)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\python312\\lib\\site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in c:\\python312\\lib\\site-packages (from yfinance) (4.3.7)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\python312\\lib\\site-packages (from yfinance) (2024.1)\n",
      "Collecting frozendict>=2.3.4 (from yfinance)\n",
      "  Using cached frozendict-2.4.6-py312-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: peewee>=3.16.2 in c:\\python312\\lib\\site-packages (from yfinance) (3.18.1)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\python312\\lib\\site-packages (from yfinance) (4.12.3)\n",
      "Collecting curl_cffi>=0.7 (from yfinance)\n",
      "  Using cached curl_cffi-0.11.3-cp39-abi3-win_amd64.whl.metadata (15 kB)\n",
      "Collecting protobuf>=3.19.0 (from yfinance)\n",
      "  Using cached protobuf-6.31.1-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: websockets>=13.0 in c:\\python312\\lib\\site-packages (from yfinance) (15.0.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\python312\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
      "Requirement already satisfied: cffi>=1.12.0 in c:\\python312\\lib\\site-packages (from curl_cffi>=0.7->yfinance) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2024.2.2 in c:\\python312\\lib\\site-packages (from curl_cffi>=0.7->yfinance) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python312\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\python312\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests>=2.31->yfinance) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests>=2.31->yfinance) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests>=2.31->yfinance) (2.4.0)\n",
      "Requirement already satisfied: pycparser in c:\\python312\\lib\\site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.16.0)\n",
      "Using cached yfinance-0.2.62-py2.py3-none-any.whl (118 kB)\n",
      "Using cached curl_cffi-0.11.3-cp39-abi3-win_amd64.whl (1.4 MB)\n",
      "Using cached frozendict-2.4.6-py312-none-any.whl (16 kB)\n",
      "Using cached protobuf-6.31.1-cp310-abi3-win_amd64.whl (435 kB)\n",
      "Installing collected packages: protobuf, frozendict, curl_cffi, yfinance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
      "ERROR: Could not install packages due to an OSError: [WinError 2] O sistema não pode encontrar o arquivo especificado: 'C:\\\\Python312\\\\Scripts\\\\sample.exe' -> 'C:\\\\Python312\\\\Scripts\\\\sample.exe.deleteme'\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cotações atualizadas com sucesso!\n",
      "Arquivo atualizado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ssl\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import yfinance as yf\n",
    "import certifi\n",
    "import warnings\n",
    "\n",
    "# Suprime o FutureWarning de auto_adjust\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Força uso do certificado correto\n",
    "os.environ['SSL_CERT_FILE'] = certifi.where()\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Lista de ativos brasileiros\n",
    "BR_TICKERS = [\n",
    "    'ALOS3', 'ABEV3', 'ASAI3', 'AURE3', 'AMOB3', 'AZUL4', 'AZZA3', 'B3SA3', 'BBSE3',\n",
    "    'BBDC3', 'BBDC4', 'BRAP4', 'BBAS3', 'BRKM5', 'BRAV3', 'BRFS3', 'BPAC11', 'CXSE3',\n",
    "    'CRFB3', 'CCRO3', 'CMIG4', 'COGN3', 'CPLE6', 'CSAN3', 'CPFE3', 'CMIN3', 'CVCB3',\n",
    "    'CYRE3', 'ELET3', 'ELET6', 'EMBR3', 'ENGI11', 'ENEV3', 'EGIE3', 'EQTL3', 'FLRY3',\n",
    "    'GGBR4', 'GOAU4', 'NTCO3', 'HAPV3', 'HYPE3', 'IGTI11', 'IRBR3', 'ISAE4', 'ITSA4',\n",
    "    'ITUB4', 'JBSS3', 'KLBN11', 'RENT3', 'LREN3', 'LWSA3', 'MGLU3', 'POMO4', 'MRFG3',\n",
    "    'BEEF3', 'MRVE3', 'MULT3', 'PCAR3', 'PETR3', 'PETR4', 'RECV3', 'PRIO3', 'PETZ3',\n",
    "    'PSSA3', 'RADL3', 'RAIZ4', 'RDOR3', 'RAIL3', 'SBSP3', 'SANB11', 'STBP3', 'SMTO3',\n",
    "    'CSNA3', 'SLCE3', 'SUZB3', 'TAEE11', 'VIVT3', 'TIMS3', 'TOTS3', 'UGPA3', 'USIM5',\n",
    "    'VALE3', 'VAMO3', 'VBBR3', 'VIVA3', 'WEGE3', 'YDUQ3'\n",
    "]\n",
    "\n",
    "def atualizar_cotacoes(file_path):\n",
    "    # Carrega CSV\n",
    "    tabela = pd.read_csv(file_path)\n",
    "\n",
    "    if 'Data' not in tabela.columns:\n",
    "        raise ValueError(\"O arquivo CSV não possui uma coluna chamada 'Data'\")\n",
    "\n",
    "    # Usa data fixa como fallback\n",
    "    data_ultima = \"02/01/2018 16:56:00\"\n",
    "    try:\n",
    "        data_datetime = datetime.strptime(data_ultima.split()[0], '%d/%m/%Y')\n",
    "    except:\n",
    "        data_datetime = datetime.now() - timedelta(days=30)\n",
    "\n",
    "    data_inicio = (data_datetime + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "    data_fim = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    colunas_ativos = tabela.columns[1:]\n",
    "    ativos_formatados = [\n",
    "        f\"{ativo}.SA\" if ativo in BR_TICKERS else ativo\n",
    "        for ativo in colunas_ativos\n",
    "    ]\n",
    "\n",
    "    batch_size = 50\n",
    "    dados_novos = pd.DataFrame()\n",
    "\n",
    "    for i in range(0, len(ativos_formatados), batch_size):\n",
    "        batch = ativos_formatados[i:i + batch_size]\n",
    "        try:\n",
    "            dados_batch = yf.download(\n",
    "                batch,\n",
    "                start=data_inicio,\n",
    "                end=data_fim,\n",
    "                interval=\"1d\",\n",
    "                auto_adjust=False,\n",
    "                progress=False\n",
    "            )['Adj Close']\n",
    "\n",
    "            if isinstance(dados_batch, pd.Series):\n",
    "                dados_batch = dados_batch.to_frame()\n",
    "\n",
    "            dados_novos = pd.concat([dados_novos, dados_batch], axis=1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao baixar lote {i//batch_size + 1}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not dados_novos.empty:\n",
    "        dados_novos.columns = [col.replace('.SA', '') for col in dados_novos.columns]\n",
    "        dados_novos.index = dados_novos.index.strftime('%d/%m/%Y %H:%M:%S')\n",
    "        dados_novos = dados_novos.ffill().bfill()\n",
    "\n",
    "        for col in dados_novos.columns:\n",
    "            dados_novos[col] = dados_novos[col].apply(\n",
    "                lambda x: f\"{x:.2f}\".replace('.', ',') if pd.notnull(x) else \"\"\n",
    "            )\n",
    "\n",
    "        dados_novos = dados_novos.reset_index().rename(columns={'Date': 'Data'})\n",
    "        tabela_atualizada = pd.concat([tabela, dados_novos], ignore_index=True)\n",
    "        tabela_atualizada.to_csv(file_path, index=False, sep=',', header=True, quoting=0)\n",
    "        print(\"Cotações atualizadas com sucesso!\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Nenhum dado novo foi encontrado.\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = 'arquivos/Dados_Ativos_B3_AdjClose.csv'\n",
    "    sucesso = atualizar_cotacoes(file_path)\n",
    "    print(\"Arquivo atualizado com sucesso!\" if sucesso else \"Houve um problema na atualização.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Arquivo criado com 503 tickers + linha de vírgulas!\n",
      "Primeira linha: ['MMM', 'AOS', 'ABT'] ...\n",
      "Segunda linha: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_sp500_tickers():\n",
    "    url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    table = soup.find('table', {'id': 'constituents'})\n",
    "    \n",
    "    tickers = []\n",
    "    for row in table.findAll('tr')[1:]:  # Pula o cabeçalho\n",
    "        ticker = row.findAll('td')[0].text.strip()\n",
    "        tickers.append(ticker)\n",
    "    \n",
    "    return tickers\n",
    "\n",
    "# Obter os tickers\n",
    "sp500_tickers = get_sp500_tickers()\n",
    "\n",
    "# Criar a linha de vírgulas (uma vírgula a menos que o número de tickers)\n",
    "linha_virgulas = [',' * (len(sp500_tickers) - 1)]\n",
    "\n",
    "# Juntar os tickers e a linha de vírgulas\n",
    "dados_para_csv = [sp500_tickers, linha_virgulas]\n",
    "\n",
    "# Criar DataFrame e salvar\n",
    "df = pd.DataFrame(dados_para_csv)\n",
    "df.to_csv('sp500_tickers_com_virgulas.csv', index=False, header=False)\n",
    "\n",
    "print(f\"✅ Arquivo criado com {len(sp500_tickers)} tickers + linha de vírgulas!\")\n",
    "print(\"Primeira linha:\", sp500_tickers[:3], \"...\")\n",
    "print(\"Segunda linha:\", linha_virgulas[0][:30] + \"...\")  # Mostra apenas parte das vírgulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[**********************50%                       ]  5 of 10 completedHTTP Error 404: \n",
      "HTTP Error 404: \n",
      "[*********************100%***********************]  10 of 10 completed\n",
      "\n",
      "10 Failed downloads:\n",
      "['ABEV3', 'BBSE3', 'ALOS3', 'AZZA3', 'AMOB3', 'AURE3', 'ASAI3', 'AZUL4', 'B3SA3', 'BBDC3']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "[*********************100%***********************]  10 of 10 completed\n",
      "\n",
      "10 Failed downloads:\n",
      "['BRAP4', 'CCRO3', 'BBDC4', 'BRKM5', 'BRAV3', 'BBAS3', 'BPAC11', 'CXSE3', 'BRFS3', 'CRFB3']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "[*********************100%***********************]  10 of 10 completed\n",
      "\n",
      "10 Failed downloads:\n",
      "['ELET6', 'CMIN3', 'CSAN3', 'ELET3', 'CYRE3', 'CPFE3', 'CMIG4', 'COGN3', 'CVCB3', 'CPLE6']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "[*********************100%***********************]  10 of 10 completed\n",
      "\n",
      "10 Failed downloads:\n",
      "['HAPV3', 'EMBR3', 'EGIE3', 'GOAU4', 'GGBR4', 'NTCO3', 'ENEV3', 'ENGI11', 'FLRY3', 'EQTL3']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "[*********************100%***********************]  9 of 10 completed\n",
      "\n",
      "10 Failed downloads:\n",
      "['IRBR3', 'HYPE3', 'KLBN11', 'ISAE4', 'RENT3', 'IGTI11', 'LREN3', 'ITUB4', 'JBSS3', 'ITSA4']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "[*********************100%***********************]  10 of 10 completed\n",
      "\n",
      "10 Failed downloads:\n",
      "['MRFG3', 'LWSA3', 'MGLU3', 'PCAR3', 'POMO4', 'PETR3', 'MRVE3', 'PETR4', 'BEEF3', 'MULT3']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "[*********************100%***********************]  10 of 10 completed\n",
      "\n",
      "10 Failed downloads:\n",
      "['RAIZ4', 'PETZ3', 'SBSP3', 'PSSA3', 'RAIL3', 'RDOR3', 'RADL3', 'RECV3', 'PRIO3', 'SANB11']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "[*********************100%***********************]  10 of 10 completed\n",
      "\n",
      "10 Failed downloads:\n",
      "['TOTS3', 'SLCE3', 'SMTO3', 'TIMS3', 'SUZB3', 'TAEE11', 'VIVT3', 'STBP3', 'CSNA3', 'UGPA3']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "[*********************100%***********************]  7 of 7 completed\n",
      "\n",
      "7 Failed downloads:\n",
      "['VAMO3', 'VIVA3', 'WEGE3', 'VALE3', 'USIM5', 'YDUQ3', 'VBBR3']: YFTzMissingError('possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nenhum dado novo foi encontrado.\n",
      "Houve um problema na atualização.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import yfinance as yf\n",
    "\n",
    "# Lista completa de ativos brasileiros (precisa de .SA)\n",
    "BR_TICKERS = [\n",
    "    'ALOS3', 'ABEV3', 'ASAI3', 'AURE3', 'AMOB3', 'AZUL4', 'AZZA3', 'B3SA3', 'BBSE3', \n",
    "    'BBDC3', 'BBDC4', 'BRAP4', 'BBAS3', 'BRKM5', 'BRAV3', 'BRFS3', 'BPAC11', 'CXSE3', \n",
    "    'CRFB3', 'CCRO3', 'CMIG4', 'COGN3', 'CPLE6', 'CSAN3', 'CPFE3', 'CMIN3', 'CVCB3', \n",
    "    'CYRE3', 'ELET3', 'ELET6', 'EMBR3', 'ENGI11', 'ENEV3', 'EGIE3', 'EQTL3', 'FLRY3', \n",
    "    'GGBR4', 'GOAU4', 'NTCO3', 'HAPV3', 'HYPE3', 'IGTI11', 'IRBR3', 'ISAE4', 'ITSA4', \n",
    "    'ITUB4', 'JBSS3', 'KLBN11', 'RENT3', 'LREN3', 'LWSA3', 'MGLU3', 'POMO4', 'MRFG3', \n",
    "    'BEEF3', 'MRVE3', 'MULT3', 'PCAR3', 'PETR3', 'PETR4', 'RECV3', 'PRIO3', 'PETZ3', \n",
    "    'PSSA3', 'RADL3', 'RAIZ4', 'RDOR3', 'RAIL3', 'SBSP3', 'SANB11', 'STBP3', 'SMTO3', \n",
    "    'CSNA3', 'SLCE3', 'SUZB3', 'TAEE11', 'VIVT3', 'TIMS3', 'TOTS3', 'UGPA3', 'USIM5', \n",
    "    'VALE3', 'VAMO3', 'VBBR3', 'VIVA3', 'WEGE3', 'YDUQ3'\n",
    "]\n",
    "\n",
    "def atualizar_cotacoes(file_path):\n",
    "    # Carregar o arquivo CSV\n",
    "    tabela = pd.read_csv(file_path)\n",
    "    \n",
    "    # Verificar coluna Data\n",
    "    if 'Data' not in tabela.columns:\n",
    "        raise ValueError(\"O arquivo CSV não possui uma coluna chamada 'Data'\")\n",
    "    \n",
    "    # Extrair dados\n",
    "    ultima_linha = tabela.iloc[-1]\n",
    "    data_ultima = \"02/01/2018 16:56:00\"\n",
    "    colunas_ativos = tabela.columns[1:]  # Excluir a coluna Data\n",
    "    \n",
    "    # Preparar tickers formatados\n",
    "    ativos_formatados = []\n",
    "    for ativo in colunas_ativos:\n",
    "        if ativo in BR_TICKERS:\n",
    "            ativos_formatados.append(ativo)  # Adiciona .SA para brasileiros\n",
    "        else:\n",
    "            ativos_formatados.append(ativo)  # Mantém original para S&P 500\n",
    "    \n",
    "    # Converter data\n",
    "    try:\n",
    "        data_datetime = datetime.strptime(data_ultima.split()[0], '%d/%m/%Y')\n",
    "        data_inicio = (data_datetime + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "    except:\n",
    "        # Caso a data já esteja em formato diferente\n",
    "        data_inicio = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    data_fim = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Baixar cotações em lotes\n",
    "    batch_size = 10  # Yahoo Finance tem limite de tickers por requisição\n",
    "    dados_novos = pd.DataFrame()\n",
    "    \n",
    "    for i in range(0, len(ativos_formatados), batch_size):\n",
    "        batch = ativos_formatados[i:i + batch_size]\n",
    "        try:\n",
    "            dados_batch = yf.download(\n",
    "                batch,\n",
    "                start=data_inicio,\n",
    "                end=data_fim,\n",
    "                interval=\"1d\",\n",
    "            )['Adj Close']\n",
    "            \n",
    "            # Corrigir formato quando há apenas 1 ativo no batch\n",
    "            if len(batch) == 1:\n",
    "                dados_batch = pd.DataFrame(dados_batch)\n",
    "                dados_batch.columns = [batch[0]]\n",
    "            \n",
    "            dados_novos = pd.concat([dados_novos, dados_batch], axis=1)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao baixar lote {i//batch_size + 1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Processar os dados\n",
    "    if not dados_novos.empty:\n",
    "        # Remover sufixos .SA dos nomes das colunas\n",
    "        dados_novos.columns = [col.replace('.SA', '') for col in dados_novos.columns]\n",
    "        \n",
    "        # Formatar datas\n",
    "        dados_novos.index = dados_novos.index.strftime('%d/%m/%Y %H:%M:%S')\n",
    "        \n",
    "        # Preencher valores ausentes\n",
    "        dados_novos = dados_novos.ffill().bfill()\n",
    "        \n",
    "        # Formatar números com 2 casas decimais e vírgula\n",
    "        for col in dados_novos.columns:\n",
    "            dados_novos[col] = dados_novos[col].apply(\n",
    "                lambda x: f\"{x:.2f}\".replace('.', ',') if pd.notnull(x) else \"\"\n",
    "            )\n",
    "        \n",
    "        # Resetar índice e renomear coluna de data\n",
    "        dados_novos = dados_novos.reset_index().rename(columns={'Date': 'Data'})\n",
    "        \n",
    "        # Concatenar com dados originais\n",
    "        tabela_atualizada = pd.concat([tabela, dados_novos], ignore_index=True)\n",
    "        \n",
    "        # Salvar arquivo\n",
    "        tabela_atualizada.to_csv(file_path, index=False, sep=',', header=True, quoting=0)\n",
    "        print(\"Cotações atualizadas com sucesso!\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Nenhum dado novo foi encontrado.\")\n",
    "        return False\n",
    "\n",
    "# Uso do código\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = 'arquivos/Dados_Ativos_B3.csv'\n",
    "    sucesso = atualizar_cotacoes(file_path)\n",
    "    \n",
    "    if sucesso:\n",
    "        print(\"Arquivo atualizado com sucesso!\")\n",
    "    else:\n",
    "        print(\"Houve um problema na atualização.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Arquivo criado com 503 linhas de 502 vírgulas cada!\n"
     ]
    }
   ],
   "source": [
    "# Número total de ativos (503)\n",
    "total_ativos = 503\n",
    "\n",
    "# Número de vírgulas por linha (502, ou seja, uma a menos que o total de ativos)\n",
    "virgulas_por_linha = total_ativos - 1\n",
    "\n",
    "# Cria 503 linhas, cada uma com 502 vírgulas\n",
    "linhas = [',' * virgulas_por_linha for _ in range(total_ativos)]\n",
    "\n",
    "# Salva em um arquivo CSV\n",
    "with open('503_linhas_virgulas.csv', 'w') as f:\n",
    "    f.write('\\n'.join(linhas))\n",
    "\n",
    "print(f\"✅ Arquivo criado com {total_ativos} linhas de {virgulas_por_linha} vírgulas cada!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\python312\\lib\\site-packages (4.21.0)\n",
      "Collecting selenium\n",
      "  Using cached selenium-4.33.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3~=2.4.0 in c:\\python312\\lib\\site-packages (from urllib3[socks]~=2.4.0->selenium) (2.4.0)\n",
      "Collecting trio~=0.30.0 (from selenium)\n",
      "  Using cached trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.12.2 (from selenium)\n",
      "  Using cached trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting certifi>=2025.4.26 (from selenium)\n",
      "  Using cached certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: typing_extensions~=4.13.2 in c:\\python312\\lib\\site-packages (from selenium) (4.13.2)\n",
      "Collecting websocket-client~=1.8.0 (from selenium)\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\python312\\lib\\site-packages (from trio~=0.30.0->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\python312\\lib\\site-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\python312\\lib\\site-packages (from trio~=0.30.0->selenium) (3.10)\n",
      "Requirement already satisfied: outcome in c:\\python312\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\python312\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\python312\\lib\\site-packages (from trio~=0.30.0->selenium) (1.16.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\python312\\lib\\site-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\python312\\lib\\site-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\python312\\lib\\site-packages (from cffi>=1.14->trio~=0.30.0->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\python312\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.14.0)\n",
      "Using cached selenium-4.33.0-py3-none-any.whl (9.4 MB)\n",
      "Using cached certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Using cached trio-0.30.0-py3-none-any.whl (499 kB)\n",
      "Using cached trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: websocket-client, certifi, trio, trio-websocket, selenium\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'Foi forçado o cancelamento de uma conexão existente pelo host remoto', None, 10054, None))': /simple/selenium/\n",
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
      "ERROR: Could not install packages due to an OSError: [WinError 2] O sistema não pode encontrar o arquivo especificado: 'C:\\\\Python312\\\\Scripts\\\\wsdump.exe' -> 'C:\\\\Python312\\\\Scripts\\\\wsdump.exe.deleteme'\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver-manager in c:\\python312\\lib\\site-packages (4.0.1)\n",
      "Collecting webdriver-manager\n",
      "  Using cached webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in c:\\python312\\lib\\site-packages (from webdriver-manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\python312\\lib\\site-packages (from webdriver-manager) (1.0.1)\n",
      "Requirement already satisfied: packaging in c:\\python312\\lib\\site-packages (from webdriver-manager) (24.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests->webdriver-manager) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests->webdriver-manager) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests->webdriver-manager) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python312\\lib\\site-packages (from requests->webdriver-manager) (2025.1.31)\n",
      "Using cached webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Installing collected packages: webdriver-manager\n",
      "  Attempting uninstall: webdriver-manager\n",
      "    Found existing installation: webdriver-manager 4.0.1\n",
      "    Uninstalling webdriver-manager-4.0.1:\n",
      "      Successfully uninstalled webdriver-manager-4.0.1\n",
      "Successfully installed webdriver-manager-4.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade selenium\n",
    "!pip install --upgrade webdriver-manager\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testes de extrair usando selenium (raspagem de tela)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'csv/TesteTotal2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Carregar o arquivo CSV\u001b[39;00m\n\u001b[32m     13\u001b[39m file_path = \u001b[33m'\u001b[39m\u001b[33mcsv/TesteTotal2.csv\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Obter os nomes dos ativos a partir das colunas do arquivo CSV, excluindo 'Data'\u001b[39;00m\n\u001b[32m     17\u001b[39m ativos = [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m data.columns \u001b[38;5;28;01mif\u001b[39;00m col != \u001b[33m'\u001b[39m\u001b[33mData\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'csv/TesteTotal2.csv'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options  \n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# Carregar o arquivo CSV\n",
    "file_path = 'csv/TesteTotal2.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Obter os nomes dos ativos a partir das colunas do arquivo CSV, excluindo 'Data'\n",
    "ativos = [col for col in data.columns if col != 'Data']\n",
    "\n",
    "# Configuração do WebDriver com modo headless\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Ativa o modo headless\n",
    "chrome_options.add_argument(\"--disable-gpu\")  # Desabilita a GPU (opcional)\n",
    "chrome_options.add_argument(\"--window-size=1920,1080\")  # Define o tamanho da janela (opcional)\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "# Dicionário para armazenar os preços dos ativos\n",
    "precos = {}\n",
    "\n",
    "# XPath para a barra de pesquisa na página inicial\n",
    "search_box_xpath = '//*[@id=\"yDmH0d\"]/c-wiz[2]/div/div[3]/div[3]/div/div/div/div[1]/input[2]'\n",
    "\n",
    "try:\n",
    "    # Loop através dos ativos\n",
    "    for ativo in ativos:\n",
    "        # Abrir o site do Google Finance\n",
    "        driver.get(\"https://www.google.com/finance/\")\n",
    "\n",
    "        # Tentar encontrar a barra de pesquisa usando o XPath\n",
    "        try:\n",
    "            search_box = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, search_box_xpath))\n",
    "            )\n",
    "        except:\n",
    "            raise Exception(f\"Barra de pesquisa não encontrada para o ativo: {ativo}\")\n",
    "\n",
    "        # Limpar o campo de busca antes de cada uso\n",
    "        search_box.clear()\n",
    "\n",
    "        # Inserir o nome do ativo no campo de busca\n",
    "        search_box.send_keys(ativo)\n",
    "\n",
    "        # Simular o pressionamento da tecla Enter\n",
    "        search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "        # Esperar o valor carregar e obter o preço\n",
    "        try:\n",
    "            value_element = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"P6K39c\"))\n",
    "            )\n",
    "        except:\n",
    "            raise Exception(f\"Preço do ativo {ativo} não encontrado.\")\n",
    "\n",
    "        # Obter o texto do preço e remover 'R$' e possíveis espaços\n",
    "        preco_texto = value_element.text.replace('R$', '').strip()\n",
    "        # Armazenar o preço como string no dicionário\n",
    "        precos[ativo] = preco_texto\n",
    "        print(f\"{ativo}: {precos[ativo]}\")\n",
    "\n",
    "    # Adicionar a data e os preços no DataFrame\n",
    "    data_atual = datetime.now()\n",
    "    data_atual = data_atual.strftime('%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "    nova_linha = {'Data': data_atual}\n",
    "    for ativo, preco in precos.items():\n",
    "        nova_linha[ativo] = preco\n",
    "\n",
    "    # Adicionar a nova linha ao DataFrame\n",
    "    data = pd.concat([data, pd.DataFrame([nova_linha])], ignore_index=True)\n",
    "\n",
    "finally:\n",
    "    # Fechar o navegador\n",
    "    driver.quit()\n",
    "\n",
    "# Salvar o arquivo atualizado\n",
    "data.to_csv(file_path, index=False)\n",
    "\n",
    "# Mostrar o DataFrame atualizado\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "url = 'https://www.b3.com.br/pt_br/market-data-e-indices/indices/indices-amplos/indice-ibovespa-ibovespa-composicao-da-carteira.htm'\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "WebDriverWait(driver, 10).until(\n",
    "    EC.frame_to_be_available_and_switch_to_it((By.ID, \"bvmf_iframe\")))\n",
    "\n",
    "planilha = driver.find_element(\"xpath\", '//*[@id=\"divContainerIframeB3\"]/div/div[1]/form/div[2]/div/div[2]/div/div/div[1]/div[2]/p/a')\n",
    "\n",
    "driver.execute_script(\"arguments[0].click();\", planilha)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Arquivo 'codigos_ibovespa.csv' criado com sucesso!\n",
      "Total de códigos extraídos: 87\n",
      "Exemplo dos primeiros códigos: ['ALOS3', 'ABEV3', 'ASAI3', 'AURE3', 'AMOB3']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lê o arquivo CSV\n",
    "df = pd.read_csv('IBOVDia_26-03-25.csv', sep=';', encoding='utf-8')\n",
    "\n",
    "# Extrai a coluna \"Aux\" e remove as últimas 2 linhas\n",
    "codigos_ativos = df['Aux'].tolist()[:-2]\n",
    "\n",
    "# Cria um DataFrame onde cada código é uma coluna\n",
    "df_output = pd.DataFrame([codigos_ativos])\n",
    "\n",
    "# Salva como CSV sem cabeçalho e sem índice\n",
    "df_output.to_csv('codigos_ibovespa.csv', index=False, header=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"✅ Arquivo 'codigos_ibovespa.csv' criado com sucesso!\")\n",
    "print(f\"Total de códigos extraídos: {len(codigos_ativos)}\")\n",
    "print(\"Exemplo dos primeiros códigos:\", codigos_ativos[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "def selecionar_ativos(data, limite=10):\n",
    "\n",
    "    \"\"\"\n",
    "    Exibe os ativos disponíveis e permite que o usuário selecione até um número limitado de ativos.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - data: DataFrame com os dados dos ativos, deve conter uma coluna 'Data'.\n",
    "    - limite: número máximo de ativos a serem selecionados (padrão: 10).\n",
    "    \n",
    "    Retorna:\n",
    "    - Lista com os nomes dos ativos selecionados.\n",
    "    \"\"\"\n",
    "    all_assets = [col for col in data.columns if col != 'Data']\n",
    "    print(\"Ativos disponíveis:\")\n",
    "    for i, asset in enumerate(all_assets):\n",
    "        print(f\"{i + 1}: {asset}\")\n",
    "\n",
    "    while True:\n",
    "        selected_indices = input(f\"Digite os números dos ativos que deseja usar (máx {limite}), separados por vírgula: \")\n",
    "        selected_indices = selected_indices.split(',')\n",
    "\n",
    "        try:\n",
    "            selected_indices = [int(i.strip()) - 1 for i in selected_indices]\n",
    "            if len(selected_indices) > limite:\n",
    "                print(f\"Você selecionou mais de {limite} ativos. Por favor, selecione no máximo {limite}.\")\n",
    "                continue\n",
    "            if any(i < 0 or i >= len(all_assets) for i in selected_indices):\n",
    "                print(\"Algum índice está fora do intervalo. Tente novamente.\")\n",
    "                continue\n",
    "            break\n",
    "        except ValueError:\n",
    "            print(\"Entrada inválida. Digite os números separados por vírgula.\")\n",
    "\n",
    "    asset_columns = [all_assets[i] for i in selected_indices]\n",
    "    print(f\"\\nAtivos selecionados: {asset_columns}\")\n",
    "    return asset_columns\n",
    "\n",
    "# Definir o caminho para os arquivos CSV\n",
    "file_path = 'arquivos/TesteTotal2.csv'\n",
    "cdi_file_path = 'arquivos/cdi_data_total.csv'\n",
    "\n",
    "# Carregar os dados do CDI e ATIVOS do arquivo CSV completo\n",
    "cdi_data_total = pd.read_csv(cdi_file_path)\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Carregar os dados históricos dos ativos\n",
    "data['Data'] = pd.to_datetime(data['Data'], format='%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "# Converter a coluna 'data' para datetime e adicionar o horário \"16:56:00\"\n",
    "cdi_data_total['data'] = pd.to_datetime(cdi_data_total['data'], dayfirst=True) + pd.Timedelta(hours=16, minutes=56)\n",
    "\n",
    "# Remove o caractere de separador de milhar e converte as colunas de ativos para float\n",
    "asset_columns = selecionar_ativos(data)\n",
    "\n",
    "for col in asset_columns:\n",
    "    data[col] = data[col].str.replace(',', '.').astype(float)\n",
    "\n",
    "# Inicializa variáveis\n",
    "aporte_inicial = 1000\n",
    "aporte_mensal = 400\n",
    "total_investido = 0\n",
    "quantities = {asset: 0 for asset in asset_columns}\n",
    "\n",
    "# ----------------------------\n",
    "# Inicialização de variáveis para o portfólio otimizado\n",
    "# ----------------------------\n",
    "\n",
    "n_assets = len(asset_columns)\n",
    "x_initial = np.ones(n_assets) / n_assets\n",
    "b = np.ones(n_assets) / n_assets\n",
    "total_invested_portfolio = 0\n",
    "quantities_portfolio = {asset: 0 for asset in asset_columns}\n",
    "# Para cada ativo, manter quantidades separadas\n",
    "quantities_individual = {asset: 0 for asset in asset_columns}\n",
    "total_invested_individual = {asset: 0 for asset in asset_columns}\n",
    "\n",
    "# Listas para armazenar os valores dos portfólios de cada estratégia\n",
    "portfolio_values_eficiente = []\n",
    "portfolio_values_paridade = []\n",
    "individual_portfolio_values = {asset: [] for asset in asset_columns}\n",
    "monthly_risk_parity_values = []  # Lista para armazenar os valores mensais da Paridade de Risco\n",
    "\n",
    "# ----------------------------\n",
    "# Funções Auxiliares\n",
    "# ----------------------------\n",
    "\n",
    "def filter_cdi_data(month, df_cdi_total, df_assets):\n",
    "    one_year_before = month - pd.DateOffset(years=1)\n",
    "    \n",
    "    # Filtrar pelo período de 1 ano antes até o mês atual\n",
    "    cdi_filtered = df_cdi_total[(df_cdi_total['data'] >= one_year_before) & (cdi_data_total['data'] < month)]\n",
    "\n",
    "    # Filtrar para garantir que as datas do CDI correspondam às do arquivo de ativos\n",
    "    cdi_filtered = cdi_filtered[cdi_filtered['data'].isin(df_assets['Data'])]\n",
    "\n",
    "    return cdi_filtered\n",
    "\n",
    "def get_yearly_data(month, data):\n",
    "    one_year_before = month - pd.DateOffset(years=1)\n",
    "    yearly_data = data[(data['Data'] >= one_year_before) & (data['Data'] < month)]\n",
    "    return yearly_data\n",
    "\n",
    "def calculate_investment(total_portfolio_value, weights, current_prices, asset_columns):\n",
    "    quantities = {}\n",
    "    for i, asset in enumerate(asset_columns):\n",
    "        current_price = current_prices[asset]\n",
    "        amount_to_invest_in_asset = total_portfolio_value * weights[i]\n",
    "        if current_price > 0:\n",
    "            quantities[asset] = amount_to_invest_in_asset / current_price\n",
    "        else:\n",
    "            quantities[asset] = 0\n",
    "    return quantities\n",
    "\n",
    "# ------------------------\n",
    "# Funções de Verificação\n",
    "# ------------------------\n",
    "\n",
    "def calculate_risk_contributions(x, cov_matrix):\n",
    "    sigma_x = np.dot(cov_matrix, x)\n",
    "    total_risk = np.sqrt(np.dot(x.T, sigma_x))\n",
    "    marginal_risk = sigma_x / total_risk\n",
    "    risk_contributions = x * marginal_risk\n",
    "    return risk_contributions, total_risk\n",
    "\n",
    "def check_equal_risk_contributions(risk_contributions):\n",
    "    total_risk = np.sum(risk_contributions)\n",
    "    n_assets = len(risk_contributions)\n",
    "    target_risk = total_risk / n_assets\n",
    "    return np.all(np.isclose(risk_contributions, target_risk, atol=1e-3))\n",
    "\n",
    "# ----------------------------\n",
    "# Funções de otimização\n",
    "# ----------------------------\n",
    "\n",
    "def calculate_variance(x, cov_matrix):\n",
    "    return np.dot(x.T, np.dot(cov_matrix, x))\n",
    "\n",
    "def objective(x, cov_matrix, b):\n",
    "    variance = calculate_variance(x, cov_matrix)\n",
    "    sqrt_variance = np.sqrt(variance)\n",
    "    w = x / sqrt_variance\n",
    "    quadratic_term = 0.5 * np.dot(w.T, b / w)\n",
    "    log_term = np.dot(b.T, np.log(w))\n",
    "    return quadratic_term - log_term\n",
    "\n",
    "def weight_sum_constraint(x):\n",
    "    return np.sum(x) - 1.0\n",
    "\n",
    "def weight_bounds(n_assets):\n",
    "    return [(1e-19, 1) for _ in range(n_assets)]\n",
    "81\n",
    "def solve_system_8(cov_matrix, b, initial_x):\n",
    "    n_assets = len(b)\n",
    "    constraints = {'type': 'eq', 'fun': weight_sum_constraint}\n",
    "    bounds = weight_bounds(n_assets)\n",
    "    solution = minimize(objective, initial_x, args=(cov_matrix, b),\n",
    "                        method='SLSQP', bounds=bounds, constraints=constraints, options={'maxiter': 1000, 'ftol': 1e-9})\n",
    "    if not solution.success:\n",
    "        raise ValueError(f\"Falha na otimização: {solution.message}\")\n",
    "    return solution.x, solution.fun\n",
    "\n",
    "# ----------------------------\n",
    "# Estratégias de Investimento\n",
    "# ----------------------------\n",
    "\n",
    "def EstrategiaEficiente(aporte_inicial, aporte_mensal, total_investido, portfolio_values_eficiente):\n",
    "    global quantities  # Para manter as quantidades atualizadas fora da função\n",
    "    print(2023)\n",
    "    print(start_2023)\n",
    "    for month in pd.date_range(start=start_2023, end=end_2023, freq='MS'):\n",
    "\n",
    "        # Definir a data de investimento como o primeiro dia útil do mês em um horário específico\n",
    "        investment_date = month + pd.offsets.BMonthBegin(0)\n",
    "        investment_datetime = investment_date.replace(hour=16, minute=56, second=0)\n",
    "        one_year_before = investment_date - pd.DateOffset(years=1)\n",
    "\n",
    "        yearly_data = data[(data['Data'] >= one_year_before) & (data['Data'] < investment_date)]\n",
    "        yearly_dataaux = data[(data['Data'] >= one_year_before) & (data['Data'] <= investment_datetime)]\n",
    "        # Filtrar os dados de CDI para o período relevante (um ano antes do mês atual) e garantir que as datas correspondam ao arquivo de ativos\n",
    "        cdidiario_filtered = filter_cdi_data(month, cdi_data_total, yearly_data)\n",
    "        \n",
    "        # Cálculo de retornos diários para todos os ativos\n",
    "        returns = yearly_data[asset_columns].pct_change().dropna()\n",
    "\n",
    "        if returns.empty:\n",
    "            print(f\"Erro: Não há retornos suficientes para calcular a matriz de covariância em {month.strftime('%m/%Y')}. Pulando este mês.\")\n",
    "            continue\n",
    "\n",
    "        # Cálculo da matriz de covariância entre todos os ativos\n",
    "        cov_matrix = returns.cov()\n",
    "\n",
    "        # Cálculo das médias dos retornos para todos os ativos\n",
    "        expected_returns = returns.mean().values\n",
    "\n",
    "        # Vetor elementar e transposto para múltiplos ativos\n",
    "        e_transposto = np.ones(len(asset_columns)).reshape(1, -1)\n",
    "\n",
    "        # Inversa da matriz de covariância\n",
    "        try:\n",
    "            cov_matrix_inv = np.linalg.inv(cov_matrix)\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(f\"Erro: Matriz de covariância singular em {month.strftime('%m/%Y')}. Pulando este mês.\")\n",
    "            continue\n",
    "\n",
    "        # Calcular a média do retorno do CDI filtrado\n",
    "        CDIRet = cdidiario_filtered['valor'].mean()\n",
    "\n",
    "        # Cálculo de Rf_e para múltiplos ativos (usando o retorno do CDI)\n",
    "        Rf_e = np.full((len(asset_columns), 1), CDIRet / 100)\n",
    "\n",
    "        # Cálculo de M - Rf_e\n",
    "        M_Rfe = expected_returns.reshape(-1, 1) - Rf_e\n",
    "\n",
    "        # Multiplicações necessárias para obter as porcentagens\n",
    "        V_M_Rfe = np.matmul(cov_matrix_inv, M_Rfe).flatten()\n",
    "        V_eT = np.matmul(e_transposto, cov_matrix_inv)\n",
    "\n",
    "        # Certifique-se de que Div seja um escalar\n",
    "        Div = np.matmul(V_eT, M_Rfe)[0, 0]\n",
    "\n",
    "        if Div == 0:\n",
    "            #print(f\"Erro: Divisor zero em {month.strftime('%m/%Y')}. Pulando este mês.\")\n",
    "            continue\n",
    "\n",
    "        # Cálculo das porcentagens para todos os ativos\n",
    "        percentages = V_M_Rfe / Div\n",
    "\n",
    "        # Determinar o valor de investimento para este mês\n",
    "        if month == pd.to_datetime(start_2023):\n",
    "            amount_to_invest = aporte_inicial\n",
    "        else:\n",
    "            amount_to_invest = aporte_mensal\n",
    "        total_investido += amount_to_invest\n",
    "        \n",
    "        # Calcular os preços atuais na data de investimento\n",
    "        current_prices = yearly_dataaux.iloc[-1][asset_columns].to_dict()\n",
    "\n",
    "        # Calcular as quantidades a serem investidas em cada ativo com base nas porcentagens otimizadas\n",
    "        new_quantities = calculate_investment(amount_to_invest, percentages, current_prices, asset_columns)\n",
    "\n",
    "        # Atualizar as quantidades do portfólio\n",
    "        for asset in asset_columns:\n",
    "            quantities[asset] += new_quantities[asset]\n",
    "\n",
    "        # Definir o início e fim do mês atual\n",
    "        start_of_month = month\n",
    "        end_of_month = month + pd.offsets.MonthEnd(0)\n",
    "\n",
    "        # Filtrar dados diários para o mês atual\n",
    "        daily_data = data[(data['Data'] >= start_of_month) & (data['Data'] <= end_of_month)].reset_index(drop=True)\n",
    "        \n",
    "        if not daily_data.empty:\n",
    "            # Substituir preços faltantes com o último preço disponível para evitar NaNs\n",
    "            daily_prices = daily_data[asset_columns].ffill()\n",
    "            \n",
    "            # --- Avaliação diária do portfólio otimizado ---\n",
    "            portfolio_daily_values = daily_prices.multiply(list(quantities.values()), axis=1).sum(axis=1)\n",
    "            portfolio_values_eficiente.extend(zip(daily_data['Data'], portfolio_daily_values))\n",
    "\n",
    "def ParidadeDeRisco(aporte_inicial, aporte_mensal, n_assets, x_initial, b, portfolio_values_paridade, individual_portfolio_values):\n",
    "    global quantities_portfolio, quantities_individual, total_invested_portfolio, monthly_risk_parity_values\n",
    "    for month in pd.date_range(start=start_2023, end=end_2023, freq='MS'):\n",
    "        \n",
    "        # Definir a data de investimento como o primeiro dia útil do mês em um horário específico\n",
    "        investment_date = month + pd.offsets.BMonthBegin(0)\n",
    "        investment_datetime = investment_date.replace(hour=16, minute=56, second=0)\n",
    "        \n",
    "        # Definir o período de retrospectiva de um ano\n",
    "        one_year_before = investment_date - pd.DateOffset(years=1)\n",
    "        \n",
    "        # Filtrar dados até a data de investimento\n",
    "        yearly_data = data[(data['Data'] >= one_year_before) & (data['Data'] < investment_date)]\n",
    "        yearly_dataaux = data[(data['Data'] >= one_year_before) & (data['Data'] <= investment_datetime)]\n",
    "        # Garantir que haja dados suficientes para o cálculo da covariância\n",
    "        if len(yearly_data) < 2:\n",
    "            #print(f\"Erro: Não há dados suficientes para calcular a covariância até {investment_date.strftime('%d/%m/%Y')}. Pulando este mês.\")\n",
    "            continue\n",
    "        # Calcular a matriz de covariância\n",
    "        cov_matrix = yearly_data[asset_columns].cov() * 1e-2  \n",
    "        \n",
    "        # Verificar se a matriz de covariância é singular\n",
    "        if cov_matrix.isnull().values.any():\n",
    "            #print(f\"Erro: Matriz de covariância contém valores NaN em {month.strftime('%m/%Y')}. Pulando este mês.\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            cov_matrix_np = cov_matrix.values\n",
    "            # Otimizar para encontrar os pesos ótimos\n",
    "            optimal_x, _ = solve_system_8(cov_matrix_np, b, x_initial)\n",
    "            #print(optimal_x * 1000)\n",
    "        except ValueError as e:\n",
    "            #print(e)\n",
    "            continue\n",
    "        x_initial = optimal_x  # Atualizar os pesos iniciais para a próxima iteração\n",
    "        \n",
    "        # Determinar o valor de investimento para este mês\n",
    "        \n",
    "        if month == pd.to_datetime(start_2023):\n",
    "            amount_to_invest = aporte_inicial\n",
    "        else:\n",
    "            amount_to_invest = aporte_mensal\n",
    "        total_invested_portfolio += amount_to_invest\n",
    "        \n",
    "        # Calcular os preços atuais na data de investimento\n",
    "        current_prices = yearly_dataaux.iloc[-1][asset_columns].to_dict()\n",
    "        \n",
    "        # Calcular as quantidades a serem investidas em cada ativo com base nos pesos otimizados\n",
    "        new_quantities = calculate_investment(amount_to_invest, optimal_x, current_prices, asset_columns)\n",
    "        #print(new_quantities)\n",
    "        # Atualizar as quantidades do portfólio\n",
    "        for asset in asset_columns:\n",
    "            quantities_portfolio[asset] += new_quantities[asset]\n",
    "        #print(month)\n",
    "        \n",
    "        # Calcular as contribuições de risco\n",
    "        risk_contributions, total_risk = calculate_risk_contributions(optimal_x, cov_matrix_np)\n",
    "        \n",
    "        target_risk = 1 / n_assets\n",
    "        if check_equal_risk_contributions(risk_contributions):\n",
    "            pass  # Nenhuma ação necessária\n",
    "        else:\n",
    "            pass  # Nenhuma ação necessária\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Atualizações de investimentos individuais\n",
    "        # ----------------------------\n",
    "        \n",
    "        for asset in asset_columns:\n",
    "            # Alocar todo o investimento para este ativo\n",
    "            total_invested_individual[asset] += amount_to_invest\n",
    "            current_price_asset = current_prices[asset]\n",
    "            #print(amount_to_invest)\n",
    "            #if asset in ('IVVB11', 'BOVA11'):\n",
    "            #    print(current_price_asset)  \n",
    "            if current_price_asset > 0:\n",
    "                quantities_individual[asset] += amount_to_invest / current_price_asset\n",
    "            else:\n",
    "                pass  # Nenhuma ação necessária\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Avaliação diária do portfólio para o mês atual\n",
    "        # ----------------------------\n",
    "        \n",
    "        # Definir o início e fim do mês atual\n",
    "        start_of_month = month\n",
    "        end_of_month = month + pd.offsets.MonthEnd(0)\n",
    "        \n",
    "        # Filtrar dados diários para o mês atual\n",
    "        daily_data = data[(data['Data'] >= start_of_month) & (data['Data'] <= end_of_month)].reset_index(drop=True)\n",
    "        \n",
    "        if not daily_data.empty:\n",
    "            # Substituir preços faltantes com o último preço disponível para evitar NaNs\n",
    "            daily_prices = daily_data[asset_columns].ffill()\n",
    "            \n",
    "            # --- Avaliação diária do portfólio otimizado ---\n",
    "            portfolio_daily_values = daily_prices.multiply(list(quantities_portfolio.values()), axis=1).sum(axis=1)\n",
    "            portfolio_values_paridade.extend(zip(daily_data['Data'], portfolio_daily_values))\n",
    "            \n",
    "            # --- Avaliação diária dos investimentos individuais ---\n",
    "            for asset in asset_columns:\n",
    "                individual_daily_values = daily_prices[asset] * quantities_individual[asset]\n",
    "                individual_portfolio_values[asset].extend(zip(daily_data['Data'], individual_daily_values))\n",
    "            \n",
    "            # ----------------------------\n",
    "            # Registro dos valores no final do mês\n",
    "            # ----------------------------\n",
    "            \n",
    "            # Obter a data e preços do último dia do mês\n",
    "            end_of_month_date = daily_data['Data'].iloc[-1]\n",
    "            end_of_month_prices = daily_data.iloc[-1][asset_columns]\n",
    "            \n",
    "            # Calcular o valor de cada ativo no portfólio\n",
    "            asset_values = end_of_month_prices * pd.Series(quantities_portfolio)\n",
    "            \n",
    "            # Calcular o valor total do portfólio\n",
    "            total_portfolio_value = asset_values.sum()\n",
    "            \n",
    "            # Calcular as porcentagens de cada ativo no portfólio\n",
    "            asset_percentages = asset_values / total_portfolio_value\n",
    "            \n",
    "            # Armazenar os valores mensais\n",
    "            monthly_risk_parity_values.append({\n",
    "                'Date': end_of_month_date,\n",
    "                'TotalValue': total_portfolio_value,\n",
    "                'AssetValues': asset_values,\n",
    "                'AssetPercentages': asset_percentages\n",
    "            })\n",
    "        \n",
    "# ----------------------------\n",
    "# Definir Período de Análise\n",
    "# ----------------------------\n",
    "\n",
    "start_2023 = '2018-01-02'\n",
    "end_2023 = '2023-12-31'\n",
    "\n",
    "# ----------------------------\n",
    "# Executar as Estratégias\n",
    "# ----------------------------\n",
    "\n",
    "EstrategiaEficiente(aporte_inicial, aporte_mensal, total_investido, portfolio_values_eficiente)\n",
    "ParidadeDeRisco(aporte_inicial, aporte_mensal, n_assets, x_initial, b, portfolio_values_paridade, individual_portfolio_values)\n",
    "\n",
    "# Converte listas em DataFrame para visualização\n",
    "df_eficiente = pd.DataFrame(portfolio_values_eficiente, columns=['Data', 'Eficiente'])\n",
    "df_paridade = pd.DataFrame(portfolio_values_paridade, columns=['Data', 'Paridade'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz Inversa (Σ⁻¹):\n",
      "[[ 51128.09   -526.92  -7320.26 -12002.15  -6613.82   9271.62]\n",
      " [  -526.92  10633.94  -3319.34   -826.56   1151.35  -2800.97]\n",
      " [ -7320.26  -3319.34   5178.03   3694.91   2551.52  -3371.4 ]\n",
      " [-12002.15   -826.56   3694.91  12612.68   2915.68  -8298.2 ]\n",
      " [ -6613.82   1151.35   2551.52   2915.68  16667.84 -11923.46]\n",
      " [  9271.62  -2800.97  -3371.4   -8298.2  -11923.46  14948.4 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Matriz de covariância\n",
    "cov_matrix = np.array([\n",
    "    [0.000031, 0.000023, 0.000045, 0.000031, 0.000017, 0.000026],\n",
    "    [0.000023, 0.000183, 0.000166, 0.000065, 0.000062, 0.000143],\n",
    "    [0.000045, 0.000166, 0.000427, 0.000005, 0.000031, 0.000127],\n",
    "    [0.000031, 0.000065, 0.000005, 0.000227, 0.000124, 0.000219],\n",
    "    [0.000017, 0.000062, 0.000031, 0.000124, 0.000212, 0.000246],\n",
    "    [0.000026, 0.000143, 0.000127, 0.000219, 0.000246, 0.000424]\n",
    "])\n",
    "\n",
    "# Verificar se a matriz é inversível\n",
    "if np.linalg.det(cov_matrix) == 0:\n",
    "    print(\"Matriz singular: determinante zero. Não pode ser invertida.\")\n",
    "else:\n",
    "    inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "    print(\"Matriz Inversa (Σ⁻¹):\")\n",
    "    print(np.round(inv_cov_matrix, 2))  # Arredondando para 2 casas decimais"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
